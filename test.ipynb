{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T06:15:42.039749Z",
     "start_time": "2026-01-22T06:15:42.035210Z"
    }
   },
   "source": [
    "# 简易实现transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设超参数\n",
    "vocab_size = 10  # 词表大小\n",
    "d_model = 6      # 词向量维度\n",
    "seq_len = 5        # 句子长度\n",
    "batch_size = 2    # 批次大小\n",
    "\n",
    "# 定义 Embedding 层\n",
    "# 这一层的参数量是: vocab_size * d_model\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "# 1. 构造输入 (Batch_Size, Seq_Len)\n",
    "# 这里是随机生成的单词索引，范围在 [0, vocab_size-1] 之间\n",
    "input_indices = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(\"输入尺寸:\", input_indices.shape) \n",
    "\n",
    "# 2. 经过 Embedding 层\n",
    "output_vectors = embedding_layer(input_indices)\n",
    "print(\"输出尺寸:\", output_vectors.shape) # 期望是 (Batch_Size, Seq_Len, D_Model)\n",
    "# print(output_vectors)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入尺寸: torch.Size([2, 5])\n",
      "输出尺寸: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:09:18.795861Z",
     "start_time": "2026-01-22T06:09:18.789027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#接下来是实战例子演示\n",
    "#假设句子是: \"I love deep learning\"\n",
    "#词表: {\"I\":0, \"love\":1, \"deep\":2, \"learning\":3}\n",
    "#对应的索引序列是: [0, 1, 2, 3]\n",
    "str = \"I love deep learning\"\n",
    "word_to_index = {\"I\":0, \"love\":1, \"deep\":2, \"learning\":3}\n",
    "input_sentence = [word_to_index[word] for word in str.split()]\n",
    "input_tensor = torch.tensor([input_sentence])  # 添加 batch 维度,[[]]多了个[]\n",
    "print(\"输入索引:\", input_tensor)"
   ],
   "id": "da04c1caca43103a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引: tensor([[0, 1, 2, 3]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:20:25.887319Z",
     "start_time": "2026-01-22T07:20:25.869496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设超参数\n",
    "vocab_size = 5  # 词表大小\n",
    "d_model = 6       # 词向量维度\n",
    "seq_len = 5        # 句子长度\n",
    "batch_size = 2     # 批次大小\n",
    "\n",
    "# 定义 Embedding 层\n",
    "# 这一层的参数量是: vocab_size * d_model\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "# 经过 Embedding 层\n",
    "output_vectors = embedding_layer(input_tensor)\n",
    "print(\"输出词向量尺寸:\", output_vectors.shape)# 期望是 (Batch_Size, Seq_Len, D_Model)\n",
    "print(\"输出词向量:\", output_vectors)\n",
    "for i in range(len(str.split())):\n",
    "    print(f\"单词: {str.split()[i]} -> 词向量: {output_vectors[0][i].detach().numpy()}\")  \n"
   ],
   "id": "cbbaed431fc270f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出词向量尺寸: torch.Size([1, 4, 6])\n",
      "输出词向量: tensor([[[-0.0667,  0.5876, -1.1609, -0.0734, -0.7703, -0.3741],\n",
      "         [-0.2260,  0.4813, -0.6121, -1.0415,  1.5145,  0.3271],\n",
      "         [-0.3737, -1.8556,  0.9718, -0.8834,  0.6442, -0.7859],\n",
      "         [ 0.7231, -1.3994, -0.8637,  1.3661, -1.1196,  0.0040]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "单词: I -> 词向量: [-0.06668201  0.5875759  -1.1609036  -0.07340842 -0.7702971  -0.37410647]\n",
      "单词: love -> 词向量: [-0.22602133  0.4812881  -0.6121076  -1.0415363   1.5144681   0.3270686 ]\n",
      "单词: deep -> 词向量: [-0.37365475 -1.8555632   0.97181636 -0.8833915   0.64417136 -0.7858806 ]\n",
      "单词: learning -> 词向量: [ 0.7231205  -1.3993503  -0.86371547  1.3661411  -1.1196176   0.00401473]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # [-0.06668201  0.5875759  -1.1609036  -0.07340842 -0.7702971  -0.37410647]\n",
    " # [-0.22602133  0.4812881  -0.6121076  -1.0415363   1.5144681   0.3270686 ]\n",
    " # [-0.37365475 -1.8555632   0.97181636 -0.8833915   0.64417136 -0.7858806 ]\n",
    " # [ 0.7231205  -1.3993503  -0.86371547  1.3661411  -1.1196176   0.00401473]"
   ],
   "id": "24888cac756fa3b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:42:15.618636Z",
     "start_time": "2026-01-22T07:42:15.610099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pos计算PE矩阵\n",
    "import torch\n",
    "import math\n",
    "str = \"I love deep learning\"\n",
    "word_to_index = {\"I\":0, \"love\":1, \"deep\":2, \"learning\":3}\n",
    "input_sentence = [word_to_index[word] for word in str.split()]\n",
    "input_tensor = torch.tensor([input_sentence])  # 添加 batch 维度,[[]]多了个[]\n",
    "print(\"输入索引:\", input_tensor)\n",
    "# 假设参数\n",
    "seq_len = len(str.split())#词的数量 4\n",
    "# print(seq_len)\n",
    "d_model = 6  # 词向量维度\n",
    "\n",
    "# 1. 先创建一个空的 Tensor 来占位\n",
    "# 形状是 (Seq_Len, d_model)，比如 (4, 5)\n",
    "pe_tensor = torch.zeros(seq_len, d_model)\n",
    "# 2. 你的循环逻辑（填空）\n",
    "\n",
    "# 这是基础写法，好理解，但效率不高\n",
    "for pos in range(seq_len):\n",
    "    for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "            val = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "        else:\n",
    "            val = math.cos(pos / (10000 ** (2 * i / d_model))) \n",
    "            \n",
    "        # 关键步骤：把计算出的值填入 Tensor\n",
    "        pe_tensor[pos, i] = val\n",
    "\n",
    "print(\"生成的 PE 矩阵形状:\", pe_tensor.shape)\n",
    "# print(pe_tensor)\n",
    "print(\"输出词向量尺寸:\", output_vectors.shape)# 期望是 (Batch_Size, Seq_Len, D_Model)\n",
    "# 对应矩阵尺寸进行加法\n",
    "final_output =output_vectors+pe_tensor.unsqueeze(0)# unsqueeze(0) 是为了匹配 batch 维度\n",
    "print(final_output)\n"
   ],
   "id": "573a5a2d504769",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引: tensor([[0, 1, 2, 3]])\n",
      "生成的 PE 矩阵形状: torch.Size([4, 6])\n",
      "输出词向量尺寸: torch.Size([1, 4, 6])\n",
      "tensor([[[-0.0667,  1.5876, -1.1609,  0.9266, -0.7703,  0.6259],\n",
      "         [ 0.6154,  1.4802, -0.6100, -0.0415,  1.5145,  1.3271],\n",
      "         [ 0.5356, -0.8599,  0.9761,  0.1166,  0.6442,  0.2141],\n",
      "         [ 0.8642, -0.4090, -0.8573,  2.3661, -1.1196,  1.0040]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:42:18.987589Z",
     "start_time": "2026-01-22T07:42:18.972129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 优化写法，利用张量运算\n",
    "pos = torch.arange(0, seq_len).unsqueeze(1)  # (Seq_Len, 1)\n",
    "i = torch.arange(0, d_model,2).unsqueeze(0)    #生成 [0, 2, 4]\n",
    "denom = torch.pow(10000, (2 * i) / d_model)  # 分母\n",
    "tmp = pos / denom  #算出一半\n",
    "#接下来分奇偶公用的进行sin和cos\n",
    "#因为这里你自己算你下就可以找到规律是0列个1列共一个单元，2列个3列共一个单元...\n",
    "pe_tensor[:, 0::2] = torch.sin(tmp)  # 偶数位置\n",
    "pe_tensor[:, 1::2] = torch.cos(tmp)  # 奇数位置\n",
    "print(pe_tensor.shape)\n",
    "print(\"生成的 PE 矩阵形状:\", pe_tensor.shape)\n",
    "# print(pe_tensor)\n",
    "print(\"输出词向量尺寸:\", output_vectors.shape)# 期望是 (Batch_Size, Seq_Len, D_Model)\n",
    "# 对应矩阵尺寸进行加法\n",
    "final_output =output_vectors+pe_tensor.unsqueeze(0)# unsqueeze(0) 是为了匹配 batch 维度\n",
    "print(final_output)\n"
   ],
   "id": "426bc9e788064d18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "生成的 PE 矩阵形状: torch.Size([4, 6])\n",
      "输出词向量尺寸: torch.Size([1, 4, 6])\n",
      "tensor([[[-0.0667,  1.5876, -1.1609,  0.9266, -0.7703,  0.6259],\n",
      "         [ 0.6154,  1.0216, -0.6100, -0.0415,  1.5145,  1.3271],\n",
      "         [ 0.5356, -2.2717,  0.9761,  0.1166,  0.6442,  0.2141],\n",
      "         [ 0.8642, -2.3893, -0.8573,  2.3661, -1.1196,  1.0040]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:09:27.915601Z",
     "start_time": "2026-01-22T12:09:27.904743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#实现自注意力机制模块\n",
    "def Attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    计算自注意力机制,以下三个矩阵都是来自上层经过线性变换得到的\n",
    "    Q: 查询矩阵 (Batch_Size, Seq_Len, D_k)\n",
    "    K: 键矩阵 (Batch_Size, Seq_Len, D_k)\n",
    "    V: 值矩阵 (Batch_Size, Seq_Len, D_v)\n",
    "    返回:\n",
    "    输出矩阵 (Batch_Size, Seq_Len, D_v)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # 获取 D_k 的维度,因为Q是(Batch_Size, Seq_Len, D_k)最后一维表示d_k\n",
    "    # 1. 计算注意力分数，套用公式\n",
    "    #注意力分数矩阵的计算公式是 Q * K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (Batch_Size, Seq_Len, Seq_Len)\n",
    "    \n",
    "    # 2. 应用 softmax 得到注意力权重\n",
    "    attn_weights = torch.softmax(scores, dim=-1)  # (Batch_Size, Seq_Len, Seq_Len),按照列来看，这里指的是每一批次的矩阵seq_len*seq_len\n",
    "    \"\"\"\n",
    "                  看 I (Col 0)  ,看 love (Col 1)      ,看 AI (Col 2), (这是 dim=-1)\n",
    "我是 I (Row 0)        ,10.0          ,5.0                  ,2.0,\"← 这一行是 \"\"I\"\" 的视角\"\n",
    "我是 love (Row 1)     ,4.0           ,12.0                 ,8.0,\n",
    "我是 AI (Row 2)       ,1.0           ,6.0                  ,11.0,\n",
    "    \"\"\"\n",
    "    # 3. 计算加权和，公式为attn_weights * V矩阵相乘\n",
    "    output = torch.matmul(attn_weights, V)  # (Batch_Size, Seq_Len, D_v)\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    "
   ],
   "id": "280af29a5a1c3bff",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:09:29.763695Z",
     "start_time": "2026-01-22T12:09:29.728088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 测试 Attention 函数\n",
    "import torch\n",
    "import math \n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 6\n",
    "d_v = 6\n",
    "# 随机生成 Q, K, V 矩阵\n",
    "Q = torch.rand(batch_size, seq_len, d_k)\n",
    "K = torch.rand(batch_size, seq_len, d_k)\n",
    "V = torch.rand(batch_size, seq_len, d_v)\n",
    "# 计算注意力输出\n",
    "output = Attention(Q, K, V)\n",
    "print(\"注意力输出尺寸:\", output.shape)  # 期望是 (Batch_Size, Seq_Len, D_v)\n",
    "print(\"注意力输出:\", output)"
   ],
   "id": "14f8b0259d12c6ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力输出尺寸: torch.Size([2, 4, 6])\n",
      "注意力输出: tensor([[[0.3803, 0.8813, 0.4401, 0.4152, 0.4913, 0.4493],\n",
      "         [0.3890, 0.8765, 0.4447, 0.4184, 0.4722, 0.4609],\n",
      "         [0.3907, 0.8757, 0.4588, 0.4140, 0.4712, 0.4673],\n",
      "         [0.4047, 0.8729, 0.4829, 0.4090, 0.4535, 0.4848]],\n",
      "\n",
      "        [[0.2425, 0.2381, 0.7493, 0.6029, 0.4236, 0.4700],\n",
      "         [0.2375, 0.2363, 0.7496, 0.5939, 0.4271, 0.4802],\n",
      "         [0.2538, 0.2265, 0.7471, 0.5850, 0.4104, 0.4833],\n",
      "         [0.2473, 0.2302, 0.7525, 0.5877, 0.4285, 0.4835]]])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_heads = 2  # 注意力头数\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model  # 词向量维度\n",
    "        self.num_heads = num_heads  # 注意力头数\n",
    "        assert self.d_model % self.num_heads == 0, \"d_model 必须能被 num_heads 整除\"\n",
    "        self.d_k = self.d_model // self.num_heads  # 每个头的维度,平分\n",
    "        \n",
    "        # 定义线性变换层\n",
    "        self.W_Q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_K = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_V = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_O = nn.Linear(self.d_model, self.d_model)# 输出线性变换层作用是把多头拼接后的结果映射回d_model维度\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V: 输入的张量，形状通常是 [Batch_Size, Seq_Len, d_model]\n",
    "        mask: 掩码张量 (可选)\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0) # 获取批次大小\n",
    "\n",
    "        # =================================================\n",
    "        # 1. 线性变换 (Projection)\n",
    "        # =================================================\n",
    "        # 经过全连接层，形状依然是 [Batch, Seq_Len, d_model]\n",
    "        # 比如 [2, 4, 6] -> [2, 4, 6]\n",
    "        Q = self.W_Q(Q)\n",
    "        K = self.W_K(K)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        # =================================================\n",
    "        # 2. 切割与转置 (Split Heads) - 最关键的一步！\n",
    "        # =================================================\n",
    "        # 我们要把 d_model (6) 切成 num_heads (2) * d_k (3)\n",
    "        # view: [Batch, Seq_Len, d_model] -> [Batch, Seq_Len, num_heads, d_k]\n",
    "        # transpose: 交换维度 1 和 2 -> [Batch, num_heads, Seq_Len, d_k]\n",
    "        # 为什么要交换？为了让 num_heads 靠近 batch 维度，这样 PyTorch 就能并行一次算出所有头的注意力\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 此时 Q, K, V 的形状都是: [2, 2, 4, 3] (Batch, Heads, Seq, d_k)\n",
    "\n",
    "        # =================================================\n",
    "        # 3. 计算注意力分数 (Scaled Dot-Product Attention)\n",
    "        # =================================================\n",
    "        # 矩阵乘法: Q * K.T\n",
    "        # K.transpose(-2, -1) 把最后两个维度互换，变成 [..., 3, 4] 以便相乘\n",
    "        # scores 形状: [Batch, Heads, Seq_Len, Seq_Len] -> [2, 2, 4, 4]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # (可选) 如果有 mask，在这里填充负无穷\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax 归一化\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # 加权求和: weights * V\n",
    "        # [2, 2, 4, 4] * [2, 2, 4, 3] -> [2, 2, 4, 3] (context)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # =================================================\n",
    "        # 4. 拼接与还原 (Concat)\n",
    "        # =================================================\n",
    "        # 先把维度换回来: transpose(1, 2) -> [Batch, Seq_Len, num_heads, d_k]\n",
    "        context = context.transpose(1, 2)\n",
    "        \n",
    "        # contiguous() 是为了解决 transpose 后内存不连续的问题，必须加\n",
    "        # view: 把最后两个维度 (Heads, d_k) 强行捏在一起 -> d_model\n",
    "        # 结果形状: [Batch, Seq_Len, d_model] -> [2, 4, 6]\n",
    "        context = context.contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # =================================================\n",
    "        # 5. 最后的线性变换 (Output Projection)\n",
    "        # =================================================\n",
    "        # 融合多头的信息\n",
    "        output = self.W_O(context)\n",
    "\n",
    "        return output\n",
    "        \n",
    "        "
   ],
   "id": "70832e7577d961d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 1. 定义两个可学习的参数 (这就是公式里的 gamma 和 beta)\n",
    "        # nn.Parameter 表示这两个变量是需要跟着模型一起训练更新的\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))  # 初始化为 1 (不做缩放)\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))  # 初始化为 0 (不做平移)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状: [Batch, Seq_Len, d_model]\n",
    "        \n",
    "        # 2. 算均值 (沿着最后一维 d_model 算)，就是每个词向量的均值，怎么才能不迷，就是按谁算谁算谁消失\n",
    "        # keepdim=True 很重要，为了保持维度形状方便后面相减\n",
    "        mean = x.mean(-1, keepdim=True) \n",
    "        \n",
    "        # 3. 算标准差 (std)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        \n",
    "        # 4. 归一化 (减均值，除标准差)\n",
    "        # 这里的 eps 是为了防止 std 是 0 导致报错\n",
    "        output = (x - mean) / (std + self.eps)\n",
    "        \n",
    "        # 5. 加上可学习的参数 (缩放 + 平移)\n",
    "        # 这一步让模型有能力“反悔”，如果不需要归一化，它可以学出对应的 gamma 和 beta 来抵消\n",
    "        output = output * self.gamma + self.beta\n",
    "        \n",
    "        return output"
   ],
   "id": "a61d019746963b69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
